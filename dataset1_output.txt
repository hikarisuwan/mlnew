============================================================
DATASET 1: ALLOY CONDUCTIVITY CLASSIFICATION
============================================================

### Step 1: Data Preparation ###
Data loaded: 5000 samples, 11 columns

=== Data Exploration ===

Shape: (5000, 11)

Data types:
density                    float64
vacancy_content            float64
melting_temperature        float64
heat_conductivity          float64
band_gap                   float64
crystallinity_index        float64
thermal_expansion_coeff    float64
young_modulus              float64
hardness                   float64
lattice_parameter          float64
label                       object
dtype: object

Missing values:
density                    43
vacancy_content            52
melting_temperature        60
heat_conductivity          49
band_gap                   45
crystallinity_index        53
thermal_expansion_coeff    55
young_modulus              44
hardness                   53
lattice_parameter          46
label                       0
dtype: int64

Basic statistics:
           density  vacancy_content  ...     hardness  lattice_parameter
count  4957.000000      4948.000000  ...  4947.000000        4954.000000
mean      5.978204         0.049105  ...    10.480742           6.468375
std       2.316794         0.028551  ...     5.496088           2.024208
min       2.000093         0.000005  ...     1.000318           3.000733
25%       3.971216         0.024668  ...     5.740285           4.695814
50%       6.004635         0.048549  ...    10.356039           6.471498
75%       7.985471         0.073320  ...    15.319856           8.238294
max       9.997741         0.099951  ...    19.999471           9.999699

[8 rows x 10 columns]

Class distribution:
label
non-conductive    4506
conductive         494
Name: count, dtype: int64

Data cleaning:
  Original samples: 5000
  Rows with missing values dropped: 480
  Remaining samples: 4520
  Data retention: 90.4%

Label encoding: {'conductive': np.int64(0), 'non-conductive': np.int64(1)}

Data split: 3616 training samples, 904 test samples

### Step 2: Correlation Analysis ###
Saved correlation matrix to outputs/dataset1_correlation_matrix.png

=== Correlation with Target Label ===
  band_gap                      :  0.6832
  vacancy_content               :  0.0261
  heat_conductivity             :  0.0202
  crystallinity_index           :  0.0176
  young_modulus                 :  0.0013
  density                       : -0.0006
  lattice_parameter             : -0.0030
  thermal_expansion_coeff       : -0.0049
  hardness                      : -0.0112
  melting_temperature           : -0.0195

### Step 3: Training Multiple Classifiers ###

Training logistic classifier...
Training completed.

=== Logistic Performance ===
Accuracy:  1.0000
Precision: 1.0000
Recall:    1.0000
F1-Score:  1.0000

Training random_forest classifier...
Training completed.

=== Random Forest Performance ===
Accuracy:  1.0000
Precision: 1.0000
Recall:    1.0000
F1-Score:  1.0000

Training decision_tree classifier...
Training completed.

=== Decision Tree Performance ===
Accuracy:  1.0000
Precision: 1.0000
Recall:    1.0000
F1-Score:  1.0000

Training gradient_boosting classifier...
Training completed.

=== Gradient Boosting Performance ===
Accuracy:  1.0000
Precision: 1.0000
Recall:    1.0000
F1-Score:  1.0000

### Step 4: Classifier Comparison ###
Saved classifier comparison to outputs/dataset1_classifier_comparison.png

### Step 5: Feature Importance Analysis ###

Feature Importance Ranking:
   1. band_gap                      : 0.963135
   2. vacancy_content               : 0.005728
   3. melting_temperature           : 0.004840
   4. crystallinity_index           : 0.004273
   5. heat_conductivity             : 0.004184
   6. density                       : 0.003961
   7. lattice_parameter             : 0.003610
   8. hardness                      : 0.003560
   9. thermal_expansion_coeff       : 0.003417
  10. young_modulus                 : 0.003292
Saved feature importance plot to outputs/dataset1_feature_importance.png

### Step 6: Confusion Matrix ###
Saved confusion matrix to outputs/dataset1_confusion_matrix.png

### Step 7: Cost-Benefit Analysis - Reduced Feature Sets ###

Training random_forest classifier...
Training completed.
Top  1 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  2 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  3 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  4 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  5 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  6 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  7 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  8 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top  9 features: Accuracy = 1.0000

Training random_forest classifier...
Training completed.
Top 10 features: Accuracy = 1.0000

### Step 8: Final Recommendations ###

======================================================================
EXECUTIVE SUMMARY - DATASET 1
======================================================================

Maximum accuracy: 1.0000 with all 10 features
Optimal feature count: 1 features achieving 1.0000 accuracy
Cost savings: 90.0% reduction in measurements

Recommended features to measure:
  1. band_gap

Rationale:
  â€¢ Achieves >99% of maximum possible accuracy
  â€¢ Reduces measurement costs by 90.0%
  â€¢ Additional features provide diminishing returns
======================================================================

âœ… Dataset 1 analysis completed!
ğŸ“ All plots saved to 'outputs/' directory
